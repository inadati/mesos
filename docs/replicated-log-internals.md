---
title: Apache Mesos - The Mesos Replicated Log
layout: documentation
---

# MesosのReplicated-Log
Mesosには、複製されたフォールトトレラントなアペンドオンリーログを作成するためのライブラリが用意されています。Mesosマスターはこのライブラリを使用して、複製された耐久性のある方法でクラスタの状態を保存します。このライブラリは、複製されたフレームワークの状態を保存したり、一般的な[複製されたステートマシン](https://en.wikipedia.org/wiki/State_machine_replication)パターンを実装したりするフレームワークでも使用できます。

## Replicated-Logとは？

![Aurora and the Replicated Log](images/log-cluster.png)

Replicated-Logでは、ログエントリの保存は追記のみで、各ログエントリには任意のデータを含めることができます。ログは複製されるため、各ログエントリはシステム内に複数のコピーを持ちます。レプリケーションは、フォールトトレランスとハイアベイラビリティを提供します。以下の例では、Mesos上で動作するフォールト・トレラント・スケジューラ（すなわちフレームワーク）である[Apache Aurora](https://aurora.apache.org/)を使用して、典型的なレプリケートされたログのセットアップを示しています。

上図のように、複数のAuroraインスタンスが同時に動作しており（高可用性のため）、そのうちの1つがリーダーに選ばれています。Auroraが動作している各ホストには、ログのレプリカがあります。Auroraは、ログAPIを含むシンライブラリを介して、Replicated-Logにアクセスできます。

通常、リーダーはログにデータを追加する唯一のマスターです。各ログエントリは複製され、システム内のすべてのレプリカに送信されます。レプリカは強い一貫性を持っています。つまり、すべてのレプリカは、各ログ・エントリの値に同意します。ログが複製されているので、Auroraがフェイルオーバーを決定する際に、リモートホストからログをコピーする必要はありません。

## 利用例
Replicated-Logは、さまざまな分散型アプリケーションの構築に利用できます。たとえば、Auroraでは、Replicated-Logを使って、すべてのタスクの状態やジョブの設定を保存しています。また、Mesosマスターのレジストリは、Replicated-Logを活用して、クラスタ内のすべてのエージェントに関する情報を保存します。

Replicated-Logは、アプリケーションがレプリケートされた状態を強い一貫性のある方法で管理するためによく使われます。これを実現する1つの方法は、各ログエントリに状態を変更する操作を保存し、分散アプリケーションのすべてのインスタンスが同じ初期状態（例えば、空の状態）に同意することです。Replicated-Logは、各アプリケーション・インスタンスが同じ順序で同じログ・エントリのシーケンスを観察することを保証します。状態変更操作の適用が決定論的である限り、これによりすべてのアプリケーション・インスタンスが互いに一貫性を保つことができます。アプリケーションのいずれかのインスタンスがクラッシュした場合、初期状態から始めて、ログに記録されたすべての変異を順に適用することで、複製された状態の現在のバージョンを再構築することができます。

ログが大きくなりすぎた場合、アプリケーションはスナップショットを書き出し、そのスナップショット以前に発生したすべてのログエントリを削除することができます。このアプローチを使用すると、Mesosでは、Replicated-Logをバックエンドとして[分散状態](https://github.com/apache/mesos/blob/master/src/state/state.hpp)の抽象化を公開することになります。

同様に、Replicated-Logを使用して、[複製された状態のマシン](https://en.wikipedia.org/wiki/State_machine_replication)を構築することができます。このシナリオでは、各ログ・エントリはステート・マシン・コマンドを含んでいます。複製は強く一貫しているので、すべてのサーバーは同じコマンドを同じ順序で実行します。

## 実装

![Replicated Log Architecture](images/log-architecture.png)

Replicated-Logは、すべてのレプリカがすべてのログエントリの値に同意することを保証するために、[Paxosのコンセンサスアルゴリズム](https://en.wikipedia.org/wiki/Paxos_%28computer_science%29)を使用しています。これは、[このスライド](https://ramcloud.stanford.edu/~ongaro/userstudy/paxos.pdf)で説明されていることと同様です。Paxosに慣れている読者は、このセクションを読み飛ばしても構いません。

上の図は、実装の概要です。ユーザーがログにデータを追加したい場合、システムはログライターを作成します。ログ・ライターは内部でコーディネータを作成します。コーディネーターはすべてのレプリカに連絡を取り、Paxosアルゴリズムを実行して、すべてのレプリカが追記されたデータについて合意することを確認します。コーディネーターは、[プロポーザー](https://en.wikipedia.org/wiki/Paxos_%28computer_science%29)と呼ばれることもあります。

各レプリカは、ログ・エントリの配列を保持しています。配列のインデックスがログの位置になります。各ログ・エントリは、ユーザが書き込んだ値、関連するPaxosの状態、そして、このログ・エントリの値が合意されたことを意味する学習ビットの3つの要素で構成されています。したがって，我々の実装では，レプリカは[Acceptor](https://en.wikipedia.org/wiki/Paxos_%28computer_science%29)でもあり[Learner](https://en.wikipedia.org/wiki/Paxos_%28computer_science%29)でもあります。

### 1つのログエントリの合意を得るために
Paxosのラウンドは、すべてのレプリカが1つのログ・エントリの値について合意に達するのを助けることができます。ラウンドには2つのフェーズがあります：promiseフェーズとwriteフェーズです。[Paxosの原著論文](https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf)とは若干異なる用語を使用していることに注意してください。我々の実装では、原著論文のprepareおよびacceptフェーズは、それぞれ promiseおよびwriteフェーズと呼ばれています。その結果、prepareリクエスト(レスポンス)はpromiseリクエスト(レスポンス)と呼ばれ、acceptリクエスト(レスポンス)はwriteリクエスト(レスポンス)と呼ばれています。

位置pのログに値Xを追加するために、コーディネータはまず、proposal番号nを持つすべてのレプリカにpromiseリクエストをブロードキャストし、nより小さいproposal番号の要求（promise／writeリクエスト）には応答しないことをレプリカに保証してもらう。

promiseリクエストを受け取ると、各レプリカは自分のPaxosの状態をチェックし、以前に配ったpromiseに応じて、そのリクエストに安全に応答できるかどうかを判断します。レプリカがpromiseを与えることができる場合（つまり、proposal番号のチェックをパスした場合）、まずそのpromise（proposal番号n）をディスクに永続化し、promiseレスポンスを返信します。もしレプリカが以前に書き込まれていた(つまりwriteリクエストを受け入れた)場合は、以前に書き込まれた値と、そのwriteリクエストで使われたproposal番号を、これから送ろうとしているpromiseレスポンスに含める必要があります。

[クォーラム](https://en.wikipedia.org/wiki/Quorum_%28distributed_computing%29)のレプリカからpromiseレスポンスを受け取ると、コーディネーターはまず、それらのレスポンスから以前に書き込まれた値が存在するかどうかをチェックします。以前に書き込まれた値が見つかった場合は、そのログエントリに対して既に値が合意されている可能性が高いため、追加操作を続行することはできません。これはPaxosの重要なアイデアの1つで、一貫性を確保するために書き込むことができる値を制限することです。

以前に書き込まれた値が見つからない場合、コーディネーターは、値Xとproposal番号nを持つすべてのレプリカにwriteリクエストをブロードキャストします。writeリクエストを受け取ると、各レプリカは自分が与えたpromiseを再度確認し、writeリクエストのproposal番号が自分が保証したproposal番号と同じかそれ以上であれば、writeレスポンスを返します。コーディネーターがレプリカの定足数からwriteレスポンスを受け取ると、追加操作は成功します。

### Multi-Paxosを使用した追加レイテンシの最適化

Replicated-Logを実装するための1つの素朴な解決策は、各ログエントリに対して完全なPaxosラウンド（promiseフェーズとwriteフェーズ）を実行することです。[Paxosの原著論文](https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf)で議論されているように、リーダーが比較的安定している場合、Multi-Paxosを使用することで、ほとんどの追記操作でpromiseフェーズが不要となり、結果的にパフォーマンスが向上します。

これを実現するために、我々は暗黙のpromiseリクエストと呼ばれる新しいタイプのpromiseリクエストを導入しました。暗黙のpromiseリクエストは、(潜在的に無限の)ログ・エントリのセットに対するバッチ処理されたプロミス・リクエストと見なすことができます。暗黙のpromiseリクエストをブロードキャストすることは、概念的には、まだ値が合意されていないすべてのログエントリに対してpromiseリクエストをブロードキャストすることと同じです。コーディネータによってブロードキャストされた暗黙のpromiseリクエストがレプリカのクォーラムによって受け入れられた場合、このコーディネータは、まだ値が合意されていないログ・エントリに追加したい場合には、プロミス・フェーズを実行する必要はなくなる。したがって、この場合のコーディネーターは、選出された（別名、リーダー）と呼ばれ、Replicated-Logへの排他的なアクセス権を持っています。選出されたコーディネーターは、他のコーディネーターがより高いproposal番号で暗黙のpromiseリクエストをブロードキャストした場合、降格（または排他的アクセスを失う）する可能性があります。

残る問題は、まだ値が合意されていないログ・エントリをどうやって見つけるかということです。非常にシンプルな解決策があります。レプリカが暗黙のpromiseリクエストを受け入れた場合、そのレプリカの最大の既知のログ位置を応答に含めます。選出されたコーディネータは、pより大きい位置のログエントリのみを追加します。pはこれらの応答で見られるどのログ位置よりも大きいです。

Multi-Paxosは、リーダーが安定していればパフォーマンスが向上します。Replicated-Log自体はリーダーの選出を行いません。代わりに、Replicated-Logのユーザーが安定したリーダーを選ぶことに依存します。例えば、Auroraはリーダーの選出に[ZooKeeper](https://zookeeper.apache.org/)を使用しています。

### ローカル読み取りの有効化
上述したように、私たちの実装では、各レプリカはAcceptorでもあり、Learnerでもあります。各レプリカをLearnerとして扱うことで、他のレプリカを介さずにローカルな読み取りを行うことができます。あるログ・エントリの値が合意されると、コーディネータは学習済みメッセージをすべてのレプリカにブロードキャストします。レプリカは学習済みメッセージを受け取ると、対応するログ・エントリの学習済みビットを設定し、そのログ・エントリの値が合意されたことを示します。学んだビットが設定されていれば、そのログ・エントリは「学習済み」になります。コーディネータはレプリカからの確認応答を待つ必要はありません。

読み取りを実行するために、ログリーダーは基礎となるローカルレプリカを直接検索します。対応するログエントリが学習されていれば、ログリーダはその値をユーザに返すだけです。そうでなければ、合意した値を発見するためにPaxosのフルラウンドが必要です。選出されたコーディネーターがいるレプリカが、常にすべてのログ・エントリを学習していることを確認します。これは、コーディネーターが選出された後に、学習されていないログ・エントリに対してPaxosのフルラウンドを実行することで実現しています。

### ガベージコレクションによるログサイズの削減

ログが大きくなった場合、アプリケーションはログを切り詰めるという選択肢を持っています。切り捨てを実行するには、特別なログ・エントリを追加します。その値は、ユーザが切り捨てたいログの位置です。レプリカは、この特別なログ・エントリを学習すると、実際にログを切り詰めることができます。

### 固有のproposal番号

[Paxosの研究論文](https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf)の多くは、各proposal番号がグローバルに一意であり、コーディネーターは常にシステム内の他のproposal番号よりも大きいproposal番号を考え出すことができると仮定しています。しかし、これを実装することは、特に分散環境においては、簡単ではありません。[一部の研究者は、グローバルに一意なサーバIDを各proposal番号に連結することを提案しています。](https://ramcloud.stanford.edu/~ongaro/userstudy/paxos.pdf)しかし、各サーバーにグローバルに一意なIDを生成する方法はまだ明らかではありません。

我々のソリューションは、上記のような仮定をしていません。コーディネーターは、最初は任意のproposal番号を使うことができます。promiseフェーズで、レプリカがコーディネーターが使ったproposal番号よりも大きいproposal番号を知っている場合、そのレプリカはコーディネーターに最大の既知のproposal番号を送り返す。コーディネータは、より大きなproposal番号でpromiseフェーズを再試行します。

ライブロックを避けるために（例えば、2つのコーディネーターが完了した場合）、各リトライの前にTから2Tの間のランダムな遅延を注入します。Tは慎重に選ぶ必要があります。一方では、T >> ブロードキャスト時間にして、他が起動する前に1つのコーディネーターが通常タイムアウトして勝利するようにします。一方で、Tはできるだけ小さくして、待ち時間を短縮したいと考えています。現在、私たちはT = 100msを使用しています。このアイデアは、実は[Raft](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf)からの借用です。

## レプリカの自動リカバリー

上述のアルゴリズムには致命的な脆弱性があります。レプリカがディスクの故障や操作ミスのために耐久性のある状態(すなわちログファイル)を失った場合、そのレプリカが単に再起動してグループに再追加されると、ログに不整合が生じる可能性があります。オペレーターは、すべてのホストでアプリケーションを停止し、リーダーのホストからログファイルをコピーしてから、アプリケーションを再起動する必要があります。オペレーターは任意のレプリカからログファイルをコピーできないことに注意してください。なぜなら、学習していないログエントリをコピーすると、誤って不正確な値でクォーラムを組み立てる可能性があり、不整合が発生するからです。

このような状況でオペレータが介入する必要がないように、MesosのReplicated-Logには自動回復のサポートが含まれています。レプリカのクォーラムが正常に動作している限り、アプリケーションのユーザーは何の違いも感じません。

### 投票権のないレプリカ
自動回復を可能にするためには、耐久性のある状態を失ったレプリカは、再起動後にコーディネーターからのリクエストに応答することを許可すべきではないということです。そうしないと、以前のPaxos状態が失われていなければ受け入れなかったはずのpromise/writeリクエストを受け入れてしまい、ログに不整合が生じる可能性があります。

この問題を解決するために、各レプリカに新しい状態変数を導入します。通常のレプリカは「投票状態」と呼ばれ、コーディネーターからのリクエストに応えることができます。状態が保存されていないレプリカは、デフォルトで「EMPTY」状態になります。EMPTY 状態のレプリカは、コーディネータからのリクエストに応答することができません。

EMPTY状態のレプリカは、次の2つの条件を満たすと投票状態になります。

1. 他のレプリカが故障しても、残ったレプリカが学習したすべてのログ・エントリを回復できるように、十分な量の失われたログ・エントリが回復されていること。
2. コーディネータへの将来の応答が、与えたpromise（潜在的に失われたもの）に反さないもの。

以下では、この2つの条件をどのようにして実現するかについて説明します。

### キャッチアップ

上記の2つの条件を満たすために、レプリカは失われた状態を回復するためにキャッチアップを行う必要があります。言い換えれば、レプリカはPaxosラウンドを実行して、すでに合意された値を持つログ・エントリを見つけ出します。問題は、ローカル・レプリカがいくつのログ・エントリをキャッチアップすれば、上記の2つの条件を満たすことができるかということです。

その結果，beginからendまでのログ・エントリをキャッチアップすればよいことがわかりました（beginはVOTINGレプリカのクォーラムで見られる最小の位置，endはVOTINGレプリカのクォーラムで見られる最大の位置です）

ここで、正しさの議論をします。eがendよりも大きい位置にあるログエントリについては、明らかに値が合意されていません。そうでなければ、レプリカのクォーラムの中に、終了位置がendよりも大きくなるようなVOTINGレプリカが少なくとも1つは見つかるはずです。同じ理由で、コーディネーターはeの位置にあるログ・エントリに対して十分なpromiseを集めていないはずです。b の位置にあるログエントリ (b が begin よりも小さいもの) は、すでに切り捨てられていて、その切り捨てが合意されているはずです。したがって、回復するレプリカがその位置のリクエストに応答することも安全です。

### 自動初期化
空のレプリカ(EMPTY状態のレプリカ)がコーディネーターからのリクエストに応答することは認めていないので、最初は各レプリカが空なので、ブートストラップに問題が生じます。Replicated-Logは、ここで2つの選択肢を提供します。1つは、ツール(`mesos-log`)を使って、各レプリカの状態をVOTINGに設定することで、明示的にログを初期化する方法ですが、アプリケーションを設定する際に余分なステップが必要になります。

もう1つの選択肢は、自動初期化を行うことです。我々の考えは、「EMPTY状態のレプリカが、すべてのレプリカがEMPTY状態であることを発見したら、すぐにVOTING状態にすることを許可する」というものです。これは、すべてのレプリカがEMPTY状態になるのは、起動時だけだという仮定に基づいています。これは、壊滅的な障害によってすべてのレプリカが耐久性のある状態を失った場合には当てはまらないかもしれませんが、保守的なユーザが自動初期化を無効にできるのはまさにそのためです。

自動初期化を行うために、単相プロトコルを使用し、レプリカがEMPTY状態からVOTING状態に直接遷移できるようにした場合、最初はすべてのレプリカがEMPTY状態であっても、進展しない状態に陥る可能性があります。たとえば、クォーラムのサイズが2で、最初はすべてのレプリカがEMPTY状態になっているとします。すべてのレプリカがEMPTY状態であることがわかると、まず1つのレプリカがその状態をVOTINGにします。その後、投票中のレプリカも、EMPTY状態のレプリカも、どちらも進行できなくなります。この問題を解決するために、2段階のプロトコルを使用し、EMPTY状態とVOTING状態の間に中間の過渡的な状態(STARTING)を導入します。EMPTY状態のレプリカは、すべてのレプリカがEMPTY状態かSTARTING状態であることがわかれば、STARTING状態に移行することができます。STARTING状態のレプリカは、すべてのレプリカがSTARTING状態かVOTING状態のどちらかであることがわかれば、VOTING状態に移行することができます。このように、先ほどの例では、すべてのレプリカがSTARTING状態になってからVOTING状態に移行することになります。

## 非リーダーのVOTINGレプリカのキャッチアップ
Mesos 1.5.0から、非リーダーのVOTINGログレプリカから最終的に一貫した読み込みを行うことができるようになりました。これにより、リーダーではないフレームワークのレプリカに対して追加の作業を行うことが可能になります。例えば、リーダーからスタンドバイへの読み取りの負荷を軽減したり、Replicated-Logで表現されるメモリ内ストレージを「ホット」に保つことでフェイルオーバー時間を短縮したりすることができます。

最終的に一貫性のある読み取りを行うために、レプリカはキャッチアップを行い、最新のログの状態を[EMPTYレプリカのリカバリ](#キャッチアップ)と同様の方法で回復する必要があります。その後、リカバリされたポジションは、「穴」が見えることを恐れずに再生することができます。

非リーダーレプリカのキャッチアップ中に、切り捨てが起こることがあります。レプリカが開始位置と終了位置を回復した後に切り捨てが起こった場合、レプリカは切り捨てられた位置を埋めようとするかもしれませんが、これはログ再生時に矛盾したデータを生成することになります。これを防ぐために、私たちは特別な墓石フラグを使っています。これは、位置が切り捨てられ、beginを調整する必要があることをレプリカに知らせるものです。キャッチアップの最中や後にレプリカが切り捨てられないようにブロックしていません。つまり、ログの再生中に回復した位置が切り捨てられた場合、ユーザーはキャッチアップ手順を再試行する必要があるということです。

## 今後の課題

現在、Replicated-Logは、動的なクォーラムサイズの変更（再構成とも呼ばれる）をサポートしていません。再構成をサポートすることで、レプリカ用のホストの追加、移動、交換がより簡単にできるようになります。将来的には再構成をサポートする予定です。
